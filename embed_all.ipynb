{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "import clip\n",
    "import torch\n",
    "from geopy.geocoders import Nominatim\n",
    "from rocksdict import Rdict\n",
    "import faiss\n",
    "import json\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pillow_heif import register_heif_opener\n",
    "from typing import Dict, Tuple, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOSE DATABASES IF NOT DONT ALREADY IN BELOW CELLS (WHILE RUNNING IN JUPYTER ONLY, TESTING)\n",
    "image_db.close()\n",
    "face_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET DATABASES TO BLANK (FOR TESTING ONLY)\n",
    "import glob\n",
    "for file in glob.glob(\"new_db/*.faiss\"):\n",
    "    os.remove(file)\n",
    "for file in glob.glob(\"new_db/*.json\"):\n",
    "    os.remove(file)\n",
    "import shutil\n",
    "db_path = \"new_db/image_db\"\n",
    "if os.path.exists(db_path):\n",
    "    shutil.rmtree(db_path)\n",
    "db_path = \"new_db/face_db\"\n",
    "if os.path.exists(db_path):\n",
    "    shutil.rmtree(db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityapatil/GitHub/image-search/venvGlimpse/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CoreMLExecutionProvider, AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/adityapatil/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/adityapatil/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/adityapatil/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/adityapatil/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/adityapatil/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SET DEVICE INFO\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# CLIP MODEL FOR SEMANTIC EMBEDDINGS\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# INSIGHTFACE MODEL (RetinaNet + MobileFaceNet)\n",
    "face_app = FaceAnalysis(name=\"buffalo_l\", detectors=['SCRFD'])\n",
    "face_app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "# Enable HEIC support\n",
    "register_heif_opener()\n",
    "\n",
    "# Nominatim for Geolocation Address generation\n",
    "geolocator = Nominatim(user_agent=\"image_search_app\")\n",
    "\n",
    "# Elasticsearch client (NOT WORKING?)\n",
    "# es = Elasticsearch([\"http://localhost:9200\"])\n",
    "# es = Elasticsearch(\"http://localhost:9200\", node_class=RequestsHttpNode)\n",
    "# es = Elasticsearch(\"http://localhost:9200\", request_timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "\n",
    "# FAISS setup for storing/searching embeddings\n",
    "clip_dim = 512\n",
    "face_dim = 512\n",
    "clip_index = faiss.IndexFlatL2(clip_dim)\n",
    "face_index = faiss.IndexFlatL2(face_dim)  # For image-specific face embeddings\n",
    "face_db_index = faiss.IndexFlatL2(face_dim)  # Face database for clustering\n",
    "\n",
    "# FACE AND METADATA STORAGE WITH RocksDict (wrapper around RocksDB)\n",
    "# face_db = {}  # {face_id: {\"embedding\": np.array, \"name\": str or None, \"count\": int}}\n",
    "# image_db.close()\n",
    "# face_db.close()\n",
    "try:\n",
    "    image_db = Rdict(\"new_db/image_db\")  # Stores image metadata\n",
    "    face_db = Rdict(\"new_db/face_db\")  # Stores face data\n",
    "except Exception as e:\n",
    "    print(\"Looks like lock is not available, closing existing db and creating new.\")\n",
    "    image_db.close()\n",
    "    face_db.close()\n",
    "    image_db = Rdict(\"new_db/image_db\")  # Stores image metadata\n",
    "    face_db = Rdict(\"new_db/face_db\")  # Stores face data\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"ImageSamples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(image_path):\n",
    "    \"\"\"Extract date and geolocation from EXIF using Pillow.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    exif_data = img._getexif()\n",
    "    metadata = {}\n",
    "    \n",
    "    if exif_data:\n",
    "        for tag_id, value in exif_data.items():\n",
    "            tag = TAGS.get(tag_id, tag_id)\n",
    "            if tag == \"DateTimeOriginal\":\n",
    "                metadata[\"date\"] = value\n",
    "            elif tag == \"GPSInfo\":\n",
    "                gps_data = {}\n",
    "                for t in value:\n",
    "                    sub_tag = TAGS.get(t, t)\n",
    "                    gps_data[sub_tag] = value[t]\n",
    "                lat = gps_data.get(\"GPSLatitude\")\n",
    "                lon = gps_data.get(\"GPSLongitude\")\n",
    "                if lat and lon:\n",
    "                    lat = float(lat[0]) + float(lat[1])/60 + float(lat[2])/3600\n",
    "                    lon = float(lon[0]) + float(lon[1])/60 + float(lon[2])/3600\n",
    "                    if gps_data.get(\"GPSLatitudeRef\") == \"S\":\n",
    "                        lat = -lat\n",
    "                    if gps_data.get(\"GPSLongitudeRef\") == \"W\":\n",
    "                        lon = -lon\n",
    "                    metadata[\"location\"] = (lat, lon)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_location_description(lat, lon):\n",
    "    \"\"\"Convert lat/lon to human-readable location using Nominatim.\"\"\"\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), language=\"en\")\n",
    "        return location.address if location else \"Unknown\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def update_face_db(face_embedding, quality_score):\n",
    "    \"\"\"Update face database with robust embeddings.\"\"\"\n",
    "    # Search for nearest face in face_db_index\n",
    "    distances, indices = face_db_index.search(np.array([face_embedding]), k=1)\n",
    "    threshold = 0.6  # Cosine similarity threshold (tune as needed)\n",
    "    if face_db_index.ntotal == 0 or distances[0][0] > threshold:  # New face\n",
    "        face_id = str(uuid.uuid4())\n",
    "        face_db[face_id] = {\"embedding\": face_embedding, \"name\": None, \"count\": 1}\n",
    "        face_db_index.add(np.array([face_embedding]))\n",
    "    else:  # Existing face\n",
    "        face_id = list(face_db.keys())[indices[0][0]]\n",
    "        current_embedding = face_db[face_id][\"embedding\"]\n",
    "        face_db[face_id][\"count\"] += 1\n",
    "        # Update embedding if quality is high (e.g., clear image, no occlusion)\n",
    "        if quality_score > 0.9:  # Assuming quality_score from InsightFace (tune threshold)\n",
    "            face_db[face_id][\"embedding\"] = (current_embedding + face_embedding) / 2  # Average for robustness\n",
    "            face_db_index.reconstruct(indices[0][0])[:] = face_db[face_id][\"embedding\"]\n",
    "\n",
    "    return face_id\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Process a single image: extract metadata, embeddings, and index.\"\"\"\n",
    "    image_id = str(uuid.uuid4())\n",
    "    metadata = extract_metadata(image_path)\n",
    "    date = metadata.get(\"date\", \"Unknown\")\n",
    "    loc_coords = metadata.get(\"location\")\n",
    "    location_desc = get_location_description(*loc_coords) if loc_coords else \"Unknown\"\n",
    "    \n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_preprocessed = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_embedding = clip_model.encode_image(img_preprocessed).cpu().numpy().flatten()\n",
    "    \n",
    "    # Detect and embed faces\n",
    "    face_data = []\n",
    "    img_np = np.array(img)\n",
    "    faces = face_app.get(img_np)\n",
    "    for face in faces:\n",
    "        face_embedding = face.embedding\n",
    "        quality_score = face.det_score  # Detection confidence as proxy for quality\n",
    "        face_id = update_face_db(face_embedding, quality_score)\n",
    "        face_data.append({\"face_id\": face_id})\n",
    "    \n",
    "    # Store in Elasticsearch\n",
    "    doc = {\n",
    "        \"image_id\": image_id,\n",
    "        \"image_path\": image_path,\n",
    "        \"date\": date,\n",
    "        \"location\": location_desc,\n",
    "        \"tags\": [],\n",
    "        \"faces\": [{\"face_id\": f[\"face_id\"]} for f in face_data]\n",
    "    }\n",
    "    \n",
    "    # es.index(index=\"images\", id=image_id, body=doc)\n",
    "    image_db[image_id.encode()] = json.dumps(doc).encode()\n",
    "    # Store CLIP embedding in FAISS\n",
    "    clip_index.add(np.array([clip_embedding]))\n",
    "    \n",
    "    # Save mapping\n",
    "    with open(\"new_db/index_mapping.json\", \"a\") as f:\n",
    "        f.write(json.dumps({\"image_id\": image_id, \"clip_idx\": clip_index.ntotal - 1}) + \"\\n\")\n",
    "    \n",
    "    return image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ImageSamples/UQVJ8690.JPG\n",
      "Processing ImageSamples/IMG_6101.JPG\n",
      "Processing ImageSamples/KXMZ7727.JPG\n",
      "Processing ImageSamples/IMG_8065.JPG\n",
      "Processing ImageSamples/VFIE1718.JPG\n",
      "Processing ImageSamples/IMG_6100.JPG\n",
      "Processing ImageSamples/CKCX6720.JPG\n",
      "Processing ImageSamples/JXRDE9695.JPG\n",
      "Processing ImageSamples/IMG_7432.JPG\n",
      "Processing ImageSamples/IMG_E1221.JPG\n",
      "Processing ImageSamples/IMG_8270.JPG\n",
      "Processing ImageSamples/IMG_7433.JPG\n",
      "Processing ImageSamples/IMG_2365.JPG\n",
      "Processing ImageSamples/CLPG3214.JPG\n",
      "Processing ImageSamples/VZJB0118.JPG\n",
      "Processing ImageSamples/IMG_7386.JPG\n",
      "Processing ImageSamples/IMG_7423.JPG\n",
      "Processing ImageSamples/DKAQ0411.JPG\n",
      "Processing ImageSamples/IMG_7422.JPG\n",
      "Processing ImageSamples/IMG_7387.JPG\n",
      "Processing ImageSamples/IMG_4249.JPG\n",
      "Processing ImageSamples/IMG_3518.JPG\n",
      "Processing ImageSamples/IMG_1680.JPG\n",
      "Processing ImageSamples/IMG_8129.JPG\n",
      "Processing ImageSamples/IMG_6110.JPG\n",
      "Processing ImageSamples/IMG_7434.JPG\n",
      "Processing ImageSamples/IMG_7420.JPG\n",
      "Processing ImageSamples/IMG_7408.JPG\n",
      "Processing ImageSamples/IMG_7409.JPG\n",
      "Processing ImageSamples/IMG_7421.JPG\n",
      "Processing ImageSamples/IMG_7435.JPG\n",
      "Processing ImageSamples/IMG_6111.JPG\n",
      "Processing ImageSamples/IMG_2363.JPG\n",
      "Processing ImageSamples/IMG_E6089.JPG\n",
      "Processing ImageSamples/IMG_7452.JPG\n",
      "Processing ImageSamples/IMG_7453.JPG\n",
      "Processing ImageSamples/IMG_5454.JPG\n",
      "Processing ImageSamples/HFTM2552.JPG\n",
      "Processing ImageSamples/OPWK2674.JPG\n",
      "Processing ImageSamples/IMG_7444.JPG\n",
      "Processing ImageSamples/IMG_5455.JPG\n",
      "Processing ImageSamples/IMG_8429.JPG\n",
      "Processing ImageSamples/IMG_3578.JPG\n",
      "Processing ImageSamples/IMG_7454.JPG\n",
      "Processing ImageSamples/IMG_7455.JPG\n",
      "Processing ImageSamples/QCRJ1563.JPG\n",
      "Processing ImageSamples/IMG_7443.JPG\n",
      "Processing ImageSamples/IMG_6576.JPG\n",
      "Processing ImageSamples/IMG_4224.JPG\n",
      "Processing ImageSamples/IMG_7467.JPG\n",
      "Processing ImageSamples/HGZP9844.JPG\n",
      "Processing ImageSamples/IMG_7466.JPG\n",
      "Processing ImageSamples/IMG_E6095.JPG\n",
      "Processing ImageSamples/IMG_8390.JPG\n",
      "Processing ImageSamples/IMG_5461.JPG\n",
      "Processing ImageSamples/IMG_7464.JPG\n",
      "Processing ImageSamples/AHSR7479.JPG\n",
      "Processing ImageSamples/IMG_7465.JPG\n",
      "Processing ImageSamples/IMG_5460.JPG\n",
      "Processing ImageSamples/IMG_5458.JPG\n",
      "Processing ImageSamples/IMG_3612.JPG\n",
      "Processing ImageSamples/IMG_5459.JPG\n",
      "Processing ImageSamples/IMG_E8135.JPG\n",
      "Processing ImageSamples/CHIV7234.JPG\n",
      "Processing ImageSamples/IMG_1211.JPG\n",
      "Processing ImageSamples/KFTHE8926.JPG\n",
      "Processing ImageSamples/IMG_8036.JPG\n",
      "Processing ImageSamples/IMG_3611.JPG\n",
      "Processing ImageSamples/IMG_E8136.JPG\n",
      "Processing ImageSamples/EUPJ2602.JPG\n",
      "Processing ImageSamples/VWCM8166.JPG\n",
      "Processing ImageSamples/IMG_7410.JPG\n",
      "Processing ImageSamples/IMG_7376.JPG\n",
      "Processing ImageSamples/IMG_7404.JPG\n",
      "Processing ImageSamples/IMG_8291.JPG\n",
      "Processing ImageSamples/IMG_7411.JPG\n",
      "Processing ImageSamples/IMG_7377.JPG\n",
      "Processing ImageSamples/IMG_7388.JPG\n",
      "Processing ImageSamples/IMG_3529.JPG\n",
      "Processing ImageSamples/AATR5358.JPG\n",
      "Processing ImageSamples/IMG_2351.JPG\n",
      "Processing ImageSamples/IMG_6094.JPG\n",
      "Processing ImageSamples/IMG_7375.JPG\n",
      "Processing ImageSamples/IMG_8293.JPG\n",
      "Processing ImageSamples/XLUZ0526.JPG\n",
      "Processing ImageSamples/IMG_8292.JPG\n",
      "Processing ImageSamples/IMG_9365.JPG\n",
      "Processing ImageSamples/IMG_6095.JPG\n",
      "Processing ImageSamples/IMG_2378.JPG\n",
      "Processing ImageSamples/IMG_2344.JPG\n",
      "Processing ImageSamples/IMG_3516.JPG\n",
      "Processing ImageSamples/GYAQ2476.JPG\n",
      "Processing ImageSamples/IMG_1926.JPG\n",
      "Processing ImageSamples/UTOP5740.JPG\n",
      "Processing ImageSamples/IMG_7403.JPG\n",
      "Processing ImageSamples/IMG_2343.JPG\n",
      "Processing ImageSamples/IMG_6643.JPG\n",
      "Processing ImageSamples/GQEE3225.JPG\n",
      "Processing ImageSamples/IMG_2342.JPG\n"
     ]
    }
   ],
   "source": [
    "# Process all images\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        print(f\"Processing {image_path}\")\n",
    "        process_image(image_path)\n",
    "\n",
    "\n",
    "# Save indices and face database\n",
    "faiss.write_index(clip_index, \"new_db/clip_index.faiss\")\n",
    "faiss.write_index(face_db_index, \"new_db/face_db_index.faiss\")\n",
    "\n",
    "import json\n",
    "\n",
    "def handle_circular_references(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert NumPy arrays to lists\n",
    "    if isinstance(obj, dict): \n",
    "        return {k: handle_circular_references(v) for k, v in obj.items()}  # Recursively fix dicts\n",
    "    if isinstance(obj, list):\n",
    "        return [handle_circular_references(v) for v in obj]  # Recursively fix lists\n",
    "    if hasattr(obj, '__dict__'):  # Convert objects to dicts safely\n",
    "        return {k: handle_circular_references(v) for k, v in obj.__dict__.items()}\n",
    "    return str(obj)  # Convert problematic objects to strings\n",
    "\n",
    "# Now dump JSON safely\n",
    "with open(\"new_db/face_db.json\", \"w\") as f:\n",
    "    json.dump(face_db, f, default=handle_circular_references, indent=4)\n",
    "\n",
    "# Close RocksDB databases after operations\n",
    "image_db.close()\n",
    "face_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rocksdict\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Open RocksDB databases\n",
    "image_db = rocksdict.Rdict(\"new_db/image_db\")  # Stores image metadata\n",
    "face_db = rocksdict.Rdict(\"new_db/face_db\")  # Stores face data\n",
    "\n",
    "def show_top_unnamed_faces():\n",
    "    \"\"\"Show top 10 unnamed faces by frequency and display their images.\"\"\"\n",
    "    unnamed_faces = {}\n",
    "\n",
    "    for fid, data in face_db.items():\n",
    "        # Ensure data is a dict\n",
    "        if isinstance(data, bytes):  \n",
    "            data = json.loads(data.decode())  # Decode bytes to dict\n",
    "        \n",
    "        if data.get(\"name\") is None:\n",
    "            unnamed_faces[fid] = data\n",
    "    # unnamed_faces = {fid: json.loads(data) for fid, data in face_db.items() if json.loads(data)[\"name\"] is None}\n",
    "    top_faces = sorted(unnamed_faces.items(), key=lambda x: x[1][\"count\"], reverse=True)[:10]\n",
    "    \n",
    "    for face_id, data in top_faces:\n",
    "        # Search for an image containing this face ID\n",
    "        for key, img_data in image_db.items():\n",
    "            img_doc = json.loads(img_data)\n",
    "            faces = img_doc.get(\"faces\", [])\n",
    "            if any(f[\"face_id\"] == face_id for f in faces):  \n",
    "                image_path = img_doc.get(\"image_path\")\n",
    "                if not image_path:\n",
    "                    print(f\"Face ID: {face_id}, Count: {data['count']}, No image path found!\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    img = Image.open(image_path).convert(\"RGB\")  # Open image from stored path\n",
    "                    \n",
    "                    # If bounding box data exists, crop face\n",
    "                    face_bbox = next((f[\"bbox\"] for f in faces if f[\"face_id\"] == face_id), None)\n",
    "                    if face_bbox:\n",
    "                        x, y, w, h = face_bbox\n",
    "                        face_img = img.crop((x, y, x + w, y + h))\n",
    "                    else:\n",
    "                        face_img = img  # Show full image if no bounding box\n",
    "                    \n",
    "                    # Display face inline\n",
    "                    plt.figure(figsize=(2, 2))\n",
    "                    plt.imshow(face_img)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.show()\n",
    "\n",
    "                    print(f\"Face ID: {face_id}, Count: {data['count']}, Image: {image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {image_path}: {e}\")\n",
    "                \n",
    "                break  # Stop after finding the first relevant image\n",
    "\n",
    "def rename_faces(face_id_name_pairs):\n",
    "    \"\"\"Rename specified face IDs.\"\"\"\n",
    "    for face_id, name in face_id_name_pairs.items():\n",
    "        face_id_encoded = face_id.encode()\n",
    "        \n",
    "        if face_id_encoded in face_db:\n",
    "            face_data = face_db[face_id_encoded]\n",
    "            \n",
    "            if isinstance(face_data, bytes):  \n",
    "                face_data = json.loads(face_data.decode())  # Decode bytes to dict\n",
    "            \n",
    "            face_data[\"name\"] = name\n",
    "            face_db[face_id_encoded] = json.dumps(face_data).encode()  # Store back in RocksDB\n",
    "            print(f\"Renamed {face_id} to {name}\")\n",
    "\n",
    "# Close RocksDB databases after operations\n",
    "image_db.close()\n",
    "face_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /Users/adityapatil/Glimpse/ImageSamples/IMG_8065.JPG: 'bbox'\n",
      "Face ID: 082686fe-4436-4c81-a430-c156905a5018, Count: 1, No image path found!\n",
      "Face ID: 166bb535-1ff6-44c8-8d61-1a0dc07a0d7e, Count: 1, No image path found!\n",
      "Face ID: 3097c8de-457b-48af-9f73-809a0ee96428, Count: 1, No image path found!\n",
      "Error loading image /Users/adityapatil/Glimpse/ImageSamples/KXMZ7727.JPG: 'bbox'\n",
      "Face ID: 3a1408e4-ee14-4c0f-8f9e-5bc6b78a1535, Count: 1, No image path found!\n",
      "Error loading image /Users/adityapatil/Glimpse/ImageSamples/IMG_6101.JPG: 'bbox'\n",
      "Face ID: 50bdc786-996a-4eb6-8558-4bd43a963756, Count: 1, No image path found!\n",
      "Error loading image /Users/adityapatil/Glimpse/ImageSamples/KXMZ7727.JPG: 'bbox'\n",
      "Face ID: 606f776b-f4b6-4559-8753-1ba9b1c78224, Count: 1, No image path found!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Open RocksDB databases\n",
    "image_db = rocksdict.Rdict(\"new_db/image_db\")  # Stores image metadata\n",
    "face_db = rocksdict.Rdict(\"new_db/face_db\")  # Stores face data\n",
    "show_top_unnamed_faces()\n",
    "image_db.close()\n",
    "face_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rename_faces({\"face_id_1\": \"Emma\", \"face_id_2\": \"John\"})  # Replace with actual IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query: str):\n",
    "    \"\"\"Simple query parser.\"\"\"\n",
    "    parts = query.lower().split()\n",
    "    semantic = []\n",
    "    metadata = {\"location\": None, \"date\": None, \"face\": None}\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"in\" and i + 1 < len(parts):\n",
    "            metadata[\"location\"] = parts[i + 1]\n",
    "        elif part == \"from\" and i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "            metadata[\"date\"] = parts[i + 1]\n",
    "        elif part == \"with\" and i + 1 < len(parts):\n",
    "            metadata[\"face\"] = parts[i + 1]\n",
    "        else:\n",
    "            semantic.append(part)\n",
    "    \n",
    "    return \" \".join(semantic), metadata\n",
    "\n",
    "index_mapping = {}\n",
    "with open(\"new_db/index_mapping.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        index_mapping[data[\"image_id\"]] = {\"clip_idx\": data[\"clip_idx\"]}\n",
    "\n",
    "# with open(\"new_db/face_db.json\", \"r\") as f:\n",
    "#     face_db = json.load(f)\n",
    "#     for fid, data in face_db.items():\n",
    "#         face_db[fid][\"embedding\"] = np.array(data[\"embedding\"])\n",
    "\n",
    "# with open(\"new_db/image_db.json\", \"r\") as f:\n",
    "#     metadata_db = json.load(f)\n",
    "#     for iid, data in metadata_db.items():\n",
    "#         metadata_db[iid][\"embedding\"] = np.array(data[\"embedding\"])\n",
    "\n",
    "def resolve_face_id(name):\n",
    "    \"\"\"\n",
    "    Returns a list of face_ids in face_db where the name matches name_to_find.\n",
    "    \"\"\"\n",
    "    return [face_id for face_id, data in face_db_index.items() if data[\"name\"] == name]\n",
    "\n",
    "\n",
    "def search(query: str) -> List[dict]:\n",
    "    \"\"\"Search images based on query.\"\"\"\n",
    "    semantic_query, metadata_filters = parse_query(query)\n",
    "    \n",
    "    # Semantic search with CLIP\n",
    "    text_input = clip.tokenize([semantic_query]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embedding = clip_model.encode_text(text_input).cpu().numpy().flatten()\n",
    "    distances, indices = clip_index.search(np.array([text_embedding]), k=100)\n",
    "    \n",
    "    # Get candidate image IDs\n",
    "    candidate_ids = [k for k, v in index_mapping.items() if v[\"clip_idx\"] in indices[0]]\n",
    "    # print(candidate_ids)\n",
    "\n",
    "    # Metadata filtering using Rocksdict\n",
    "    filtered_ids = []\n",
    "    for image_id in candidate_ids:\n",
    "        doc = image_db.get(image_id.encode())\n",
    "        if doc:\n",
    "            doc = json.loads(doc.decode())\n",
    "            # Apply filters\n",
    "            if metadata_filters[\"location\"].lower() not in doc[\"location\"].lower():\n",
    "                continue\n",
    "            print(\"found location\")\n",
    "            if metadata_filters[\"date\"].lower() not in doc[\"date\"].lower():\n",
    "                continue\n",
    "            print(\"found date\")\n",
    "            if metadata_filters[\"face\"]:\n",
    "                face_id = resolve_face_id(metadata_filters[\"face\"])\n",
    "                if face_id and face_id not in [f[\"face_id\"] for f in doc[\"faces\"]]:\n",
    "                    continue\n",
    "                print(\"found face\")\n",
    "\n",
    "            filtered_ids.append(image_id)\n",
    "    \n",
    "    # Ranking\n",
    "    results = []\n",
    "    for image_id in filtered_ids:\n",
    "        clip_idx = index_mapping[image_id][\"clip_idx\"]\n",
    "        score = 1 - distances[0][list(indices[0]).index(clip_idx)]\n",
    "        results.append({\"image_id\": image_id, \"score\": score, \"metadata\": json.loads(image_db[image_id.encode()])})\n",
    "    \n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_db = Rdict(\"new_db/image_db\")  # Stores image metadata\n",
    "\n",
    "query = \"photos in Mumbai\"\n",
    "results = search(query)\n",
    "\n",
    "image_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Kerala\" and \"Kerala\" != \"Kerala\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"KERALA\".lower() in \"Varkala, Kerala, India\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvGlimpse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
